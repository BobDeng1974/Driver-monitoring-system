{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "Requirement already satisfied: python-dateutil in /glob/intel-python/versions/2018u2/intelpython3/lib/python3.6/site-packages (from kaggle)\n",
      "Requirement already satisfied: requests in /glob/intel-python/versions/2018u2/intelpython3/lib/python3.6/site-packages (from kaggle)\n",
      "Requirement already satisfied: six>=1.10 in /glob/intel-python/versions/2018u2/intelpython3/lib/python3.6/site-packages (from kaggle)\n",
      "Requirement already satisfied: certifi in /glob/intel-python/versions/2018u2/intelpython3/lib/python3.6/site-packages (from kaggle)\n",
      "Collecting tqdm (from kaggle)\n",
      "  Using cached https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /glob/intel-python/versions/2018u2/intelpython3/lib/python3.6/site-packages (from kaggle)\n",
      "Collecting python-slugify (from kaggle)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /glob/intel-python/versions/2018u2/intelpython3/lib/python3.6/site-packages (from requests->kaggle)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /glob/intel-python/versions/2018u2/intelpython3/lib/python3.6/site-packages (from requests->kaggle)\n",
      "Collecting text-unidecode==1.2 (from python-slugify->kaggle)\n",
      "  Using cached https://files.pythonhosted.org/packages/79/42/d717cc2b4520fb09e45b344b1b0b4e81aa672001dd128c180fabc655c341/text_unidecode-1.2-py2.py3-none-any.whl\n",
      "Installing collected packages: tqdm, text-unidecode, python-slugify, kaggle\n",
      "Successfully installed kaggle-1.5.3 python-slugify-3.0.0 text-unidecode-1.2 tqdm-4.31.1\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/u25322/.kaggle/’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ~/kaggle.json ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/u25322/.kaggle/kaggle.json'\n",
      "Downloading sample_submission.csv.zip to /home/u25322\n",
      "100%|████████████████████████████████████████| 193k/193k [00:00<00:00, 1.28MB/s]\n",
      "\n",
      "Downloading imgs.zip to /home/u25322\n",
      "100%|█████████████████████████████████████▉| 4.00G/4.00G [02:38<00:00, 29.1MB/s]\n",
      "100%|██████████████████████████████████████| 4.00G/4.00G [02:40<00:00, 26.8MB/s]\n",
      "Downloading driver_imgs_list.csv.zip to /home/u25322\n",
      "  0%|                                               | 0.00/92.9k [00:00<?, ?B/s]\n",
      "100%|██████████████████████████████████████| 92.9k/92.9k [00:00<00:00, 4.44MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c state-farm-distracted-driver-detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip_ref = zipfile.ZipFile('./imgs.zip', 'r')\n",
    "zip_ref.extractall('./')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get each image filepath into a list\n",
    "img_list = glob.glob(\"/home/tmbluth/Documents/GitHub/Distracted-Drivers-Detection/train/**/*.jpg\")\n",
    "img_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in ALL train images and label them\n",
    "actual_images = []\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for j in range(10):\n",
    "    print('Load folder c{}'.format(j))\n",
    "    path = os.path.join('.', 'train', 'c' + str(j), '*.jpg')\n",
    "    img_list = glob.glob(path)\n",
    "    for file in img_list:\n",
    "        # Open each image and convert to black and white\n",
    "        img = Image.open(file).convert('L')\n",
    "        # Get image height and width to divide each by 2 (save memory)\n",
    "        height, width = img.size\n",
    "        # thumbnail is an in-place operation\n",
    "        img.thumbnail((height/4, width/4), Image.ANTIALIAS)\n",
    "        # Make each image into a numpy array divided by the max to get 0 to 1 normalization\n",
    "        img_data = np.array(img.getdata()) / 255\n",
    "        # Grow lists of actual images, image data, and labels\n",
    "        actual_images.append(img)\n",
    "        X.append(img_data)\n",
    "        y.append(j)\n",
    "# Combine list of images into numpy array\n",
    "X = np.array(X, dtype = 'float32')    \n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print()\n",
    "print(\"Loading and cleaning images took %s\" % str(end - start), \"seconds\", '\\n')\n",
    "print(\"Number of train images: %s\" % len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y[0:20000:5000])\n",
    "actual_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_h5 = h5py.File('X.h5', 'w')\n",
    "X_h5.create_dataset('X_h5', data=X)\n",
    "X_h5.close()\n",
    "\n",
    "y_h5 = h5py.File('y.h5', 'w')\n",
    "y_h5.create_dataset('y_h5', data=y)\n",
    "y_h5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, Callback\n",
    "from keras.layers import Conv2D, Dense, Dropout, BatchNormalization, MaxPooling2D, Activation, Flatten\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import pickle\n",
    "\n",
    "# This make sure the input shape is: (number of samples, width, height, channels)\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "# Load in preprocessed data\n",
    "with h5py.File('X.h5', 'r') as hf:\n",
    "    X = hf['X_h5'][:]\n",
    "with h5py.File('y.h5', 'r') as hf:\n",
    "    y = hf['y_h5'][:]\n",
    "\n",
    "# Get train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, shuffle = True)\n",
    "\n",
    "# Prepare images to correct shape for Keras model\n",
    "X_train = X_train.reshape(X_train.shape[0], 160, 120, 1)\n",
    "X_test= X_test.reshape(X_test.shape[0], 160, 120, 1)\n",
    "y_train = to_categorical(y_train).astype('int32')\n",
    "y_test = to_categorical(y_test).astype('int32')\n",
    "\n",
    "# Check data to see if everything is ok\n",
    "print(\" 'X' matrix/input \")\n",
    "print(\"Shape: \", str(X_train.shape))\n",
    "print(X_train[0][0][:5], '\\n')\n",
    "print(\" 'y' label/output \")\n",
    "print(\"Shape: \", str(y_train.shape))\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDRScheduler(Callback):\n",
    "    '''Cosine annealing learning rate scheduler with periodic restarts.\n",
    "    # Arguments\n",
    "        min_lr: The lower bound of the learning rate range for the experiment.\n",
    "        max_lr: The upper bound of the learning rate range for the experiment.\n",
    "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
    "        lr_decay: Reduce the max_lr after the completion of each cycle.\n",
    "                  Ex. To reduce the max_lr by 20% after each cycle, set this value to 0.8.\n",
    "        cycle_length: Initial number of epochs in a cycle.\n",
    "        mult_factor: Scale epochs_to_restart after each full cycle completion.\n",
    "\n",
    "    # References\n",
    "        Blog post: jeremyjordan.me/nn-learning-rate\n",
    "        Original paper: http://arxiv.org/abs/1608.03983\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 min_lr,\n",
    "                 max_lr,\n",
    "                 steps_per_epoch,\n",
    "                 lr_decay=1,\n",
    "                 cycle_length=10,\n",
    "                 mult_factor=2):\n",
    "\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.lr_decay = lr_decay\n",
    "\n",
    "        self.batch_since_restart = 0\n",
    "        self.next_restart = cycle_length\n",
    "\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "        self.cycle_length = cycle_length\n",
    "        self.mult_factor = mult_factor\n",
    "\n",
    "        self.history = {}\n",
    "\n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
    "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n",
    "        return lr\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.max_lr)\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        self.batch_since_restart += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        '''Check for end of current cycle, apply restarts when necessary.'''\n",
    "        if epoch + 1 == self.next_restart:\n",
    "            self.batch_since_restart = 0\n",
    "            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n",
    "            self.next_restart += self.cycle_length\n",
    "            self.max_lr *= self.lr_decay\n",
    "            self.best_weights = self.model.get_weights()\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n",
    "        self.model.set_weights(self.best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Use next line if not doing cosine annealing\n",
    "# early_stop = EarlyStopping(monitor = 'val_loss', patience = 3)\n",
    "\n",
    "schedule = SGDRScheduler(min_lr=1e-4,\n",
    "                         max_lr=1e-2,\n",
    "                         steps_per_epoch=np.ceil(X_train.shape[0]/batch_size),\n",
    "                         lr_decay=0.8,\n",
    "                         cycle_length=5,\n",
    "                         mult_factor=2)\n",
    "\n",
    "def DD_VGG():\n",
    "    model = Sequential()\n",
    "    # Use Batch Normalization for every conv and dense layers\n",
    "    model.add(Conv2D(64, kernel_size = (3,3), activation = 'relu', input_shape = (160, 120, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, kernel_size = (3,3), activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(128, kernel_size = (3,3), activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, kernel_size = (3,3), activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.33))\n",
    "    model.add(Dense(32, activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(10, activation = 'softmax')) # Sigmoid for log loss?\n",
    "    model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "    return model\n",
    "\n",
    "driver_model = DD_VGG()\n",
    "driver_hist = driver_model.fit(X_train, y_train, epochs=15, batch_size=batch_size, verbose=1,\n",
    "                               validation_split=0.2, callbacks=[schedule])\n",
    "\n",
    "def plot_model(model_hist, train_metric, val_metric):\n",
    "    plt.plot(model_hist[train_metric])\n",
    "    plt.plot(model_hist[val_metric])\n",
    "    plt.ylabel('Model ' + str(train_metric))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "plot_model(driver_hist.history, 'loss', 'val_loss')\n",
    "plot_model(driver_hist.history, 'acc', 'val_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model when retraining\n",
    "driver_model.save('./driver_model.h5')\n",
    "#driver_model = load_model('./driver_model.h5')\n",
    "\n",
    "# Save model history\n",
    "with open('./trainHistoryDict', 'wb') as history:\n",
    "        pickle.dump(driver_hist.history, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions and make them into vector of 0 to 9\n",
    "test_pred = driver_model.predict(X_test)\n",
    "test_pred2 = np.argmax(test_pred, axis = 1)\n",
    "\n",
    "# Change dummy code matrix back to vector of 0 to 9\n",
    "y_test2 = np.argmax(y_test, axis = 1)\n",
    "\n",
    "# Get performance\n",
    "print(confusion_matrix(y_test2, test_pred2))\n",
    "print(\"Test set accuracy is: %s\" % accuracy_score(y_test2, test_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "\n",
    "# Re-import needed modules\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "\n",
    "submission_data = []\n",
    "filenames = []\n",
    "\n",
    "img_list = sorted(glob.glob(\"/home/tmbluth/Documents/GitHub/Distracted-Drivers-Detection/test/*.jpg\"))\n",
    "for file in img_list:\n",
    "     # Open each image and convert to black and white\n",
    "    img = Image.open(file)\n",
    "    filenames.append(os.path.basename(img.filename))\n",
    "    img = img.convert('L')\n",
    "    # Get image height and width to divide each by 4 (save memory)\n",
    "    # thumbnail is an in-place operation\n",
    "    img.thumbnail((img.size[0]/4, img.size[1]/4), Image.ANTIALIAS)\n",
    "    # Make into numpy array divided by the max to get 0 to 1 normalization\n",
    "    img_data = np.array(img.getdata()) / 255\n",
    "    submission_data.append(img_data)\n",
    "\n",
    "print()\n",
    "print(\"Number of test images: %s\" % len(submission_data))\n",
    "\n",
    "# Make list into numpy array and reshape to feed into model\n",
    "submission_data = np.array(submission_data, dtype = 'float32')\n",
    "submission_data = submission_data.reshape(submission_data.shape[0], 160, 120, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in model and use it to predict\n",
    "driver_model = load_model('./driver_model.h5')\n",
    "\n",
    "# Get log probabilities\n",
    "kaggle_log_probs = driver_model.predict(submission_data)\n",
    "kaggle_log_probs = pd.DataFrame(kaggle_log_probs, index = filenames, columns = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\n",
    "\n",
    "kaggle_log_probs.to_csv('./DD_kaggle_submission.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "kaggle_submission = pd.read_csv('./DD_kaggle_submission.csv')\n",
    "\n",
    "print(kaggle_submission.head(10))\n",
    "print(kaggle_submission.info())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel, 2018 update 2)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_2018u2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
